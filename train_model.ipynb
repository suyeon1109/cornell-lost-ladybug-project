{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suyeon1109/cornell-lost-ladybug-project/blob/main/train_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zibuUoWm2vag",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "outputId": "473049a5-b62c-4b70-f1cf-0ee94dac2d06"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-967125014d4a>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'FolderName'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ImageFileName'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Array'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Modify columns as needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mfolder_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllp_photos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mfolder_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllp_photos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# if os.path.isdir(folder_path):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/AI image/photos'"
          ]
        }
      ],
      "source": [
        "'''\n",
        "빈 csv 만들기 (칼럼 두개, 이미지 파일명 - 어레이)\n",
        "폴더 안의 폴더 안의 이미지들을 불러올 수 있는 반복문\n",
        "이미지 파일명 칼럼에 이미지 파일명 저장\n",
        "케라스 method 써서 이미지를 어레이로 변환\n",
        "csv 에 두번쨰 칼럼에 어레이 add\n",
        "Metadata csv 랑 이미지 파일명으로 merge\n",
        "'''\n",
        "\n",
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from keras.utils import load_img, img_to_array\n",
        "from csv import writer\n",
        "\n",
        "# Set the paths\n",
        "llp_photos = '/content/drive/MyDrive/AI image/photos'\n",
        "# output_csv = '/content/image_in_array.csv'\n",
        "\n",
        "# Create an empty DataFrame to store the data\n",
        "data = pd.DataFrame(columns=['FolderName','ImageFileName', 'Array'])  # Modify columns as needed\n",
        "\n",
        "for folder_name in os.listdir(llp_photos):\n",
        "    folder_path = os.path.join(llp_photos, folder_name)\n",
        "    # if os.path.isdir(folder_path):\n",
        "    # print(folder_path)\n",
        "    file_list = os.listdir(folder_path)\n",
        "    file_count = len(file_list)\n",
        "    # print(file_count)\n",
        "\n",
        "    # 이미지를 다 어레이로 바꾸기 전에 전처리에서 걸러질 애들 최대한 미리 버리려고 했는데 ipynb_checkpoints 문서 때문에 실패 (코랩/주피터 노트북 쓸때 생기는 거 같음)\n",
        "    # if file_count < 100 and folder_path!=\"/content/drive/MyDrive/AI image/photos/.ipynb_checkpoints\":\n",
        "    #     # print(folder_path)\n",
        "    #     # print(file_count)\n",
        "    #     for image_name in os.listdir(folder_path):\n",
        "    #         image_path = os.path.join(folder_path, image_name)\n",
        "    #         # print(image_path)\n",
        "    #         os.remove(image_path)\n",
        "    #         print(file_count)\n",
        "        # os.rmdir(folder_path)\n",
        "    for image_name in os.listdir(folder_path):\n",
        "        try:\n",
        "            # print(image_name)\n",
        "            image_path = os.path.join(folder_path, image_name)\n",
        "            # print(image_path)\n",
        "            img = load_img(image_path,target_size=(160,160))\n",
        "            # print(img)\n",
        "            img_array = img_to_array(img)\n",
        "            # print(img_array)\n",
        "            # print(img_array.shape)\n",
        "            # allow_pickle=True\n",
        "            # print(image_array)\n",
        "\n",
        "            row_input = [folder_name, image_name, img_array]\n",
        "\n",
        "            # Open our existing CSV file in append mode\n",
        "            # Create a file object for this file\n",
        "            with open('/content/image_in_array.csv', 'a') as output_csv:\n",
        "\n",
        "                # Pass this file object to csv.writer()\n",
        "                # and get a writer object\n",
        "                writer_object = writer(output_csv)\n",
        "\n",
        "                # Pass the list as an argument into\n",
        "                # the writerow()\n",
        "                writer_object.writerow(row_input)\n",
        "\n",
        "                # Close the file object\n",
        "                output_csv.close()\n",
        "                print(row_input)\n",
        "        except:\n",
        "              pass\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "id": "9m8wrRRNDWnn",
        "outputId": "bac36d9c-4940-4591-f972-2fe53820236b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-3f786850e387>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Set the paths\n",
        "llp_photos = '/content/drive/MyDrive/AI image/photos'\n",
        "\n",
        "folders_to_save = []\n",
        "for folder_name in os.listdir(llp_photos):\n",
        "    folder_path = os.path.join(llp_photos, folder_name)\n",
        "\n",
        "    file_list = os.listdir(folder_path)\n",
        "    file_count = len(file_list)\n",
        "    if file_count >= 100:\n",
        "        folders_to_save.append(folder_name)\n",
        "print(folders_to_save)"
      ],
      "metadata": {
        "id": "7XfexE4hGLV_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9edd8b30-c72d-44e4-eabb-232a89d15c99"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Adalia+bipunctata', 'Anatis+labiculata', 'Anatis+lecontei', 'Anatis+mali', 'Brachiacantha+ursina', 'Coccinella+californica', 'Coccinella+septempunctata', 'Coccinella+transversoguttata', 'Coccinella+novemnotata', 'Cycloneda+munda', 'Cycloneda+polita', 'Coleomegilla+maculata', 'Coccinella+trifasciata', 'Diomus+terminatus', 'Coelophora+inaequalis', 'Cycloneda+sanguinea', 'Exochomus+aethiops', 'Hippodamia+convergens', 'Harmonia+', 'Hippodamia+caseyi', 'Hippodamia+parenthesis', 'Hippodamia+spp.', 'Hippodamia+variegata', 'Hippodamia+tredecimpunctata', 'Olla+v-nigrum', 'Psyllobora+vigintimaculata', 'Propylea+quatuordecimpunctata']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "folders_to_save = ['Cycloneda+polita', 'Coccinella+monticola', 'Cycloneda+munda', 'Olla+v-nigrum', 'Coccinella+californica', 'Coccinella+septempunctata', 'Hippodamia+variegata', 'Coelophora+inaequalis', 'Exochomus+aethiops', 'Hippodamia+parenthesis', 'Adalia+bipunctata', 'Anatis+labiculata', 'Hippodamia+convergens', 'Hippodamia+spp.', 'Cycloneda+sanguinea', 'Hippodamia+tredecimpunctata', 'Coccinella+trifasciata', 'Harmonia+', 'Chilocorus+stigma', 'Coccinella+novemnotata', 'Psyllobora+vigintimaculata', 'Propylea+quatuordecimpunctata', 'Coleomegilla+maculata', 'Hippodamia+caseyi', 'Anatis+mali', 'Coccinella+transversoguttata']\n",
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "# main_path='/content/image_in_array.csv'\n",
        "original = pd.read_csv(\n",
        "    '/content/final_img_ary.csv'\n",
        ")\n",
        "\n",
        "original['species'] = original['species'].astype(str)\n",
        "original['imageFileName'] = original['imageFileName'].astype(str)\n",
        "\n",
        "filtered_rows = []\n",
        "with open('/content/final_img_ary.csv', 'r') as original:\n",
        "    original_csv = csv.reader(original)\n",
        "    for row in original_csv:\n",
        "        if any(value in row for value in folders_to_save):\n",
        "            filtered_rows.append(row)\n",
        "            print(row)\n",
        "\n",
        "# Write the filtered rows to a new CSV file\n",
        "with open('/content/dropped.csv', 'w', newline='') as dropped:\n",
        "    csv_writer = csv.writer(dropped)\n",
        "    csv_writer.writerows(filtered_rows)\n",
        "\n",
        "dropped.info()"
      ],
      "metadata": {
        "id": "o9xXSnrEIbPK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "outputId": "bff15030-629d-464e-961d-ff7bf1d745c5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-d8a0d727f16c>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# main_path='/content/image_in_array.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m original = pd.read_csv(\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;34m'/content/final_img_ary.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/final_img_ary.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(folders_to_save)"
      ],
      "metadata": {
        "id": "4NUuD0ifiOUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_ary = pd.read_csv(\n",
        "    '/content/dropped.csv')"
      ],
      "metadata": {
        "id": "qUttEIuSaggZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_ary.rename(columns = {\n",
        "    'Cycloneda+polita': 'species',\n",
        "    '346_2_M.jpg': 'imageFileName',\n",
        "    str(img_ary.columns.tolist()[1]): 'array',\n",
        "    }\n",
        "    , inplace = True)\n",
        "\n",
        "metad.rename(columns = {\n",
        "    'photoFilename': 'imageFileName',\n",
        "    }\n",
        "    , inplace = True)\n",
        "metad['imageFileName'] = data['imageFileName'].str.replace('_O.', '_M.')\n",
        "img_ary.info()\n",
        "# img_ary\n",
        "metad.info()\n",
        "print(metad['imageFileName'])\n",
        "\n",
        "merged = img_ary.merge(metad, on='imageFileName', how='inner')\n",
        "merged.info()"
      ],
      "metadata": {
        "id": "kKI0vlOeahr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "columns = merged.columns\n",
        "empty_df = pd.DataFrame(columns=columns)\n",
        "\n",
        "for i in merged.speciesName.unique():\n",
        "    filtered_indices = merged[merged['speciesName'] == i].index.tolist()\n",
        "    stack1 = merged.iloc[filtered_indices]\n",
        "    sampled_indices = random.choices(stack1.index.tolist(), k=111)\n",
        "    stack2 = merged.iloc[sampled_indices]\n",
        "    empty_df = pd.concat([empty_df, stack2], axis=0)"
      ],
      "metadata": {
        "id": "7anHk2S9kQn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "empty_df"
      ],
      "metadata": {
        "id": "PQQWEeywu6Jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from sklearn.utils import shuffle\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Add a vacant column (replace 'new_column_name' with the desired column name)\n",
        "empty['label'] = ''\n",
        "\n",
        "# Define other parameters\n",
        "sample_num = 22\n",
        "\n",
        "# Preprocess data\n",
        "label_encoder = LabelEncoder()\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "images = []\n",
        "labels = []\n",
        "\n",
        "for idx, row in data.iterrows():\n",
        "    # Process image data (assuming image array column name is 'array')\n",
        "    img_array = np.fromstring(row['array'][1:-1], sep=',')\n",
        "\n",
        "    # Process string data (assuming string column name is 'speciesName')\n",
        "    string_value = row['speciesName']\n",
        "    categorical_value = label_encoder.fit_transform([string_value])[0]\n",
        "\n",
        "    # Process float data (assuming float column name is 'longitude')\n",
        "    float_value = row['longitude']\n",
        "    scaled_float = scaler.fit_transform(np.array([[float_value]]))\n",
        "\n",
        "    # Combine all processed features into a single input vector\n",
        "    input_vector = np.concatenate([img_array, [categorical_value], scaled_float.flatten()])\n",
        "\n",
        "    images.append(input_vector)\n",
        "    labels.append(row['label'])  # Assuming you have a 'label' column in your CSV\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "images = np.array(images)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Shuffle the data\n",
        "images, labels = shuffle(images, labels)\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build the model\n",
        "model = Sequential()\n",
        "model.add(Dense(64, activation='relu', input_shape=(images.shape[1],)))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(label_encoder.classes_.shape[0], activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, to_categorical(y_train), epochs=10, batch_size=100)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(X_test, to_categorical(y_test))\n",
        "print('test_acc:', test_acc)\n",
        "print('test_loss:', test_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "id": "f_i1FH5yFG-G",
        "outputId": "7feebc9c-86e4-4513-e001-fdbe220428a9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1b888f710d82>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Add a vacant column (replace 'new_column_name' with the desired column name)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mempty\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Define other parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'empty' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터를 speciesName 기준으로 나누고 저장하기\n",
        "train_data = pd.DataFrame(columns=columns)\n",
        "test_data = pd.DataFrame(columns=columns)\n",
        "\n",
        "empty_df=empty_df.reset_index()\n",
        "\n",
        "# merged 데이터프레임의 'speciesName' 열 값에 따라 데이터를 나눕니다.\n",
        "for i in empty_df.speciesName.unique():\n",
        "    # 'speciesName' 열이 i와 일치하는 행의 인덱스를 가져옵니다.\n",
        "    filtered_indices = empty_df[empty_df['speciesName'] == i].index.tolist()\n",
        "\n",
        "    # filtered_indices를 활용하여 해당 행들을 가져옵니다.\n",
        "    species_data = empty_df.loc[filtered_indices]\n",
        "\n",
        "    # 학습 데이터와 테스트 데이터로 분리 (2대8 비율)\n",
        "    num_samples = len(species_data)\n",
        "    num_train = int(num_samples * 0.8)  # 80%를 학습 데이터로 사용\n",
        "    train_indices = random.sample(filtered_indices, num_train)\n",
        "    test_indices = list(set(filtered_indices) - set(train_indices))\n",
        "\n",
        "    # 학습 데이터와 테스트 데이터를 각각 저장\n",
        "    train_data = pd.concat([train_data, empty_df.loc[train_indices]], axis=0)\n",
        "    test_data = pd.concat([test_data, empty_df.loc[test_indices]], axis=0)"
      ],
      "metadata": {
        "id": "cPmjPCNep6oq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.to_csv(\"train.csv\")\n",
        "train_data = pd.read_csv(\"train.csv\")\n",
        "\n",
        "test_data.to_csv(\"test.csv\")\n",
        "test_data = pd.read_csv(\"test.csv\")"
      ],
      "metadata": {
        "id": "kn32S_ALwEki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.info()"
      ],
      "metadata": {
        "id": "zXLarVf-p9ZH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "outputId": "4a9b21c7-a89d-42a6-de91-a4a250b519c3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-60f4b0d084c6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tensorflow.keras.layers import TextVectorization, CategoryEncoding\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Load your CSV data into a DataFrame (replace this with your data loading code)\n",
        "data = pd.read_csv('/content/train (1).csv')\n",
        "\n",
        "# Identify object and string columns you want to convert\n",
        "obj_to_str_columns = ['imageFileName', 'speciesName', 'isGuess', 'photoQuality',\n",
        "       'stateName', 'countyName', 'tribalLand','habitatName', 'timeOfDay',\n",
        "       'searchPlanned', 'method', 'weather', 'foundBugs', 'hasplant',\n",
        "       'Educational: egg', 'Educational: prey', 'Educational: larva',\n",
        "       'Educational: aggregation', 'Educational: pupa', 'Educational: not ladybug',\n",
        "       'Other: kids', 'Other: story', 'Other: rare', 'Other: out NAm',\n",
        "       'Other: Host Plant ID', 'Source: Kids', 'Source: Youth (14-23)',\n",
        "       'Source: Adult', 'Source: Group','Source: Family', 'latitude',\n",
        "       'longitude', 'temperature']\n",
        "\n",
        "str_to_int_columns = ['latitude', 'longitude', 'temperature']\n",
        "\n",
        "object_columns = data.select_dtypes(include=['object'])\n",
        "\n",
        "for column_name in obj_to_str_columns:\n",
        "    data[column_name] = pd.factorize(data[column_name])[0].astype(str)\n",
        "\n",
        "for each in str_to_int_columns:\n",
        "    data[each] = pd.to_numeric(data[each], errors='coerce').astype('Float64')\n",
        "\n",
        "\n",
        "object_columns = data.select_dtypes(include=['object'])\n",
        "\n",
        "for column_name in object_columns:\n",
        "    counter = 0\n",
        "    for unique_value in data[column_name].unique().tolist():\n",
        "        data.loc[data[column_name] == unique_value, column_name] = str(counter)\n",
        "        counter += 1\n",
        "\n",
        "\n",
        "import os\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define other parameters\n",
        "image_size = (180, 180)  # Adjust the image size as needed\n",
        "sample_num = 1000\n",
        "\n",
        "# Preprocess data\n",
        "label_encoder = LabelEncoder()\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "images = []\n",
        "labels = []\n",
        "\n",
        "for idx, row in data.iterrows():\n",
        "    # Process image data (assuming image column name is 'image_path')\n",
        "    img = image.load_img(row['array'], target_size=image_size)\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = img_array / 255.0  # Normalize pixel values\n",
        "    images.append(img_array)\n",
        "\n",
        "    # Process string data (assuming string column name is 'string_column')\n",
        "    string_value = row['speciesName']\n",
        "    categorical_value = label_encoder.fit_transform([string_value])[0]\n",
        "\n",
        "    # Process float data (assuming float column name is 'float_column')\n",
        "    float_value = row['temperature']\n",
        "    scaled_float = scaler.fit_transform(np.array(float_value).reshape(-1, 1))\n",
        "\n",
        "    # Combine all processed features into a single input vector\n",
        "    input_vector = np.concatenate([img_array.flatten(), [categorical_value], scaled_float.flatten()])\n",
        "\n",
        "    labels.append(row['label'])  # Assuming you have a 'label' column in your CSV\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "images = np.array(images)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Build the model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(label_encoder.classes_.shape[0], activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(images, to_categorical(labels), epochs=10, batch_size=100)\n",
        "\n",
        "# Evaluate the model (similar to your provided code)\n",
        "test_loss, test_acc = model.evaluate(images, to_categorical(labels))\n",
        "print('test_acc:', test_acc)\n",
        "print('test_loss:', test_loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "4B_Vg1m2z2dA",
        "outputId": "8940a875-f90d-4b17-eba5-0e6054075839"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-7cafd82730df>\u001b[0m in \u001b[0;36m<cell line: 60>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;31m# Process image data (assuming image column name is 'image_path')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'array'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0mimg_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mimg_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_array\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.0\u001b[0m  \u001b[0;31m# Normalize pixel values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/image_utils.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '0'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdaPmuDJXen1",
        "outputId": "d1eba353-a881-480e-c485-59b0ec43fcde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 6072 entries, 0 to 6071\n",
            "Data columns (total 41 columns):\n",
            " #   Column                    Non-Null Count  Dtype  \n",
            "---  ------                    --------------  -----  \n",
            " 0   Unnamed: 0.1              6072 non-null   int64  \n",
            " 1   Unnamed: 0                6072 non-null   int64  \n",
            " 2   imageFileName             6072 non-null   object \n",
            " 3   array                     6072 non-null   object \n",
            " 4   photoSubmissionID         6072 non-null   int64  \n",
            " 5   speciesName               6072 non-null   object \n",
            " 6   isGuess                   6072 non-null   object \n",
            " 7   photoQuality              6072 non-null   object \n",
            " 8   year                      6072 non-null   int64  \n",
            " 9   stateName                 6072 non-null   object \n",
            " 10  countyName                6072 non-null   object \n",
            " 11  tribalLand                6072 non-null   object \n",
            " 12  latitude                  6072 non-null   Float64\n",
            " 13  longitude                 6072 non-null   Float64\n",
            " 14  spotterNumber             6072 non-null   int64  \n",
            " 15  habitatName               6072 non-null   object \n",
            " 16  searchHours               6072 non-null   float64\n",
            " 17  timeOfDay                 6072 non-null   object \n",
            " 18  searchPlanned             6072 non-null   object \n",
            " 19  method                    6072 non-null   object \n",
            " 20  weather                   6072 non-null   object \n",
            " 21  temperature               6072 non-null   Float64\n",
            " 22  foundBugs                 6072 non-null   object \n",
            " 23  hasplant                  6072 non-null   object \n",
            " 24  Educational: egg          6072 non-null   object \n",
            " 25  Educational: prey         6072 non-null   object \n",
            " 26  Educational: larva        6072 non-null   object \n",
            " 27  Educational: aggregation  6072 non-null   object \n",
            " 28  Educational: pupa         6072 non-null   object \n",
            " 29  Educational: not ladybug  6072 non-null   object \n",
            " 30  Other: kids               6072 non-null   object \n",
            " 31  Other: story              6072 non-null   object \n",
            " 32  Other: rare               6072 non-null   object \n",
            " 33  Other: out NAm            6072 non-null   object \n",
            " 34  Other: Host Plant ID      6072 non-null   object \n",
            " 35  Source: Kids              6072 non-null   object \n",
            " 36  Source: Youth (14-23)     6072 non-null   object \n",
            " 37  Source: Adult             6072 non-null   object \n",
            " 38  Source: Group             6072 non-null   object \n",
            " 39  Source: Family            6072 non-null   object \n",
            " 40  index                     6072 non-null   float64\n",
            "dtypes: Float64(3), float64(2), int64(5), object(31)\n",
            "memory usage: 1.9+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8i9imqcF3Fw",
        "outputId": "b7db2438-a15b-439b-dc13-e7b93304b4c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    }
  ]
}